{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a9c8c3b3",
      "metadata": {
        "id": "a9c8c3b3"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## Tugas 3: Information Retrieval\n",
        "\n",
        "### Mekanisme\n",
        "\n",
        "Anda hanya diwajibkan untuk mengumpulkan file ini saja ke uploader yang disediakan di https://elearning.uai.ac.id/. Ganti nama file ini saat pengumpulan menjadi **tugas3_NIM.ipynb**.\n",
        "\n",
        "**Keterlambatan**: Pengumpulan tugas yang melebihi tenggat yang telah ditentukan tidak akan diterima. Keterlambatan akan berakibat pada nilai nol untuk tugas ini.\n",
        "\n",
        "**Kolaborasi**: Anda diperbolehkan untuk berdiskusi dengan teman Anda, tetapi dilarang keras menyalin kode maupun tulisan dari teman Anda.\n",
        "\n",
        "### Petunjuk\n",
        "\n",
        "Pastikan jawaban Anda singkat, padat, dan jelas. Mayoritas pertanyaan yang diberikan dapat dijawab dalam 3-4 kalimat saja.\n",
        "\n",
        "### Catatan ⚠️\n",
        "\n",
        "1. Anda mungkin membutuhkan akses GPU untuk mengerjakan tugas ini. Untuk memudahkan, Anda dapat menggunakan akses GPU gratis dari [Google Colab](https://colab.research.google.com/).\n",
        "2. Untuk bagian 3 dari tugas ini, Anda memerlukan akses ke model [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fc2bd648",
      "metadata": {
        "id": "fc2bd648"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f2f10e8",
      "metadata": {
        "id": "6f2f10e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6e6333-34b6-4cf0-da91-5c1918739327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
            "Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25, fastparquet\n",
            "Successfully installed fastparquet-2024.11.0 rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# Jalani cell ini untuk menginstalasi dependencies untuk memuat data dan menjalankan model\n",
        "!pip install huggingface_hub fastparquet pyarrow rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c9766c",
      "metadata": {
        "id": "06c9766c"
      },
      "source": [
        "## 1. Spare Matrix Retrieval (15 poin)\n",
        "\n",
        "*Acknowledgement:* Data yang digunakan dalam tugas ini adalah [MS MARCO](https://huggingface.co/datasets/microsoft/ms_marco), sebuah koleksi dataset yang digunakan untuk mengembangkan model deep learning dalam kasus pencarian. Kumpulan data ini adalah jawaban pertanyaan berdasarkan 100.000 pertanyaan Bing asli dan jawaban yang dibuat manusia.\n",
        "\n",
        "Untuk bagian ini, gunakan `df_train` untuk menjawab soal-soal yang diberikan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83e1cbdc",
      "metadata": {
        "id": "83e1cbdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c3b60c-ac19-45e9-f5fb-85b6b06ad798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "splits = {\n",
        "    \"validation\": \"v1.1/validation-00000-of-00001.parquet\",\n",
        "    \"train\": \"v1.1/train-00000-of-00001.parquet\",\n",
        "    \"test\": \"v1.1/test-00000-of-00001.parquet\",\n",
        "}\n",
        "df_train = pd.read_parquet(\"hf://datasets/microsoft/ms_marco/\" + splits[\"train\"])\n",
        "df_val = pd.read_parquet(\"hf://datasets/microsoft/ms_marco/\" + splits[\"validation\"])\n",
        "df_test = pd.read_parquet(\"hf://datasets/microsoft/ms_marco/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e593d4a3",
      "metadata": {
        "id": "e593d4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "bf0720d8-87c5-4f1b-c367-b3bd18e35309"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             answers  \\\n",
              "0  [Results-Based Accountability is a disciplined...   \n",
              "1                                              [Yes]   \n",
              "2                                    [20-25 minutes]   \n",
              "3                       [$11 to $22 per square foot]   \n",
              "4                      [Due to symptoms in the body]   \n",
              "\n",
              "                                            passages  \\\n",
              "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
              "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...   \n",
              "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
              "3  {'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...   \n",
              "4  {'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...   \n",
              "\n",
              "                                               query  query_id   query_type  \\\n",
              "0                                        what is rba     19699  description   \n",
              "1                       was ronald reagan a democrat     19700  description   \n",
              "2  how long do you need for sydney and surroundin...     19701      numeric   \n",
              "3                    price to install tile in shower     19702      numeric   \n",
              "4                    why conversion observed in body     19703  description   \n",
              "\n",
              "  wellFormedAnswers  \n",
              "0                []  \n",
              "1                []  \n",
              "2                []  \n",
              "3                []  \n",
              "4                []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d63286f-6eb0-4afb-890a-b5412195234b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answers</th>\n",
              "      <th>passages</th>\n",
              "      <th>query</th>\n",
              "      <th>query_id</th>\n",
              "      <th>query_type</th>\n",
              "      <th>wellFormedAnswers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Results-Based Accountability is a disciplined...</td>\n",
              "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...</td>\n",
              "      <td>what is rba</td>\n",
              "      <td>19699</td>\n",
              "      <td>description</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Yes]</td>\n",
              "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...</td>\n",
              "      <td>was ronald reagan a democrat</td>\n",
              "      <td>19700</td>\n",
              "      <td>description</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[20-25 minutes]</td>\n",
              "      <td>{'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...</td>\n",
              "      <td>how long do you need for sydney and surroundin...</td>\n",
              "      <td>19701</td>\n",
              "      <td>numeric</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[$11 to $22 per square foot]</td>\n",
              "      <td>{'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...</td>\n",
              "      <td>price to install tile in shower</td>\n",
              "      <td>19702</td>\n",
              "      <td>numeric</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Due to symptoms in the body]</td>\n",
              "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...</td>\n",
              "      <td>why conversion observed in body</td>\n",
              "      <td>19703</td>\n",
              "      <td>description</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d63286f-6eb0-4afb-890a-b5412195234b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d63286f-6eb0-4afb-890a-b5412195234b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d63286f-6eb0-4afb-890a-b5412195234b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1624a751-8705-4d92-999b-f1aa7b65242d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1624a751-8705-4d92-999b-f1aa7b65242d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1624a751-8705-4d92-999b-f1aa7b65242d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 82326,\n  \"fields\": [\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"passages\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 82326,\n        \"samples\": [\n          \"where is burnwell ala\",\n          \"what constitutes connective tissue\",\n          \"how much does a rehabilitation center employee make\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query_id\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 82326,\n        \"samples\": [\n          73880,\n          52315,\n          29597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"numeric\",\n          \"person\",\n          \"location\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wellFormedAnswers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9369114",
      "metadata": {
        "id": "a9369114"
      },
      "source": [
        "### Soal 1.1 (2 poin)\n",
        "\n",
        "Terdapat beberapa baris yang tidak memiliki passage yang relevan, i.e. semua elemen `is_selected` bernilai nol. Hapus semua baris tersebut."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b7b3b676",
      "metadata": {
        "id": "b7b3b676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "46cf70ed-8075-44bb-aa48-5ba9e5e51ecf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ellipsis' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3199389375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;31m# Ganti ini dengan kode untuk memproses df_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;31m# Ganti ini dengan kode untuk memproses df_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m79704\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9706\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "df_train = df_train[df_train['passages'].apply(lambda x: sum(x['is_selected'])) > 0]\n",
        "df_val = df_val[df_val['passages'].apply(lambda x: sum(x['is_selected'])) > 0]\n",
        "assert df_train.shape[0] == 79704\n",
        "assert df_val.shape[0] == 9706"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bec0217",
      "metadata": {
        "id": "2bec0217"
      },
      "source": [
        "### Soal 1.2 (2 poin)\n",
        "\n",
        "Lengkapi fungsi untuk menghitung average precision di bawah ini. Nilai parameter `y_true` adalah `is_selected` dari kolom `passages`, sedangkan `y_scores` adalah nilai yang digunakan untuk mengurutkan passage berdasarkan relevansi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59751a71",
      "metadata": {
        "id": "59751a71"
      },
      "outputs": [],
      "source": [
        "def average_precision(y_true, y_scores):\n",
        "    relevant_docs = np.sum(y_true)\n",
        "    if relevant_docs == 0:\n",
        "        return 0.0\n",
        "\n",
        "    sorted_indices = np.argsort(y_scores)[::-1]\n",
        "    y_true_sorted = y_true[sorted_indices]\n",
        "\n",
        "    precision_sum = 0.0\n",
        "    hits = 0\n",
        "    for i, is_relevant in enumerate(y_true_sorted):\n",
        "        if is_relevant:\n",
        "            hits += 1\n",
        "            precision_sum += hits / (i + 1)\n",
        "\n",
        "    return precision_sum / relevant_docs\n",
        "\n",
        "\n",
        "assert np.isclose(average_precision(np.array([1, 0, 1]), np.array([0.9, 0.8, 0.7])), 0.8333, atol=1e-4)\n",
        "assert np.isclose(average_precision(np.array([1, 0, 1]), np.array([0.95, 0.8, 0.9])), 1.0)\n",
        "assert np.isclose(average_precision(np.array([1, 0, 0]), np.array([0.0, 0.8, 0.9])), 1 / 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc95c1c",
      "metadata": {
        "id": "9cc95c1c"
      },
      "source": [
        "### Soal 1.3 (2 poin)\n",
        "\n",
        "Implementasikan fungsi untuk menghitung cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592f3ec0",
      "metadata": {
        "id": "592f3ec0"
      },
      "outputs": [],
      "source": [
        "def cos_sim(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    if norm_a == 0 or norm_b == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "\n",
        "assert np.isclose(cos_sim(np.array([1, 0]), np.array([1, 0])), 1.0)\n",
        "assert np.isclose(cos_sim(np.array([1, 0]), np.array([0, 1])), 0.0)\n",
        "assert np.isclose(cos_sim(np.array([1, 2]), np.array([2, 4])), 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054dd8da",
      "metadata": {
        "id": "054dd8da"
      },
      "source": [
        "### Soal 1.4 (4 poin)\n",
        "\n",
        "Gunakan metode TF-IDF untuk mengukur relevansi dari passage berdasarkan *cosine similarity* dari query dan passages. Hitung nilai *mean average precision* dari sistem yang dihasilkan. Dalam sistem ini, Anda tidak perlu membandingkan query dengan semua passages dari setiap baris di tabel. Anda hanya perlu membandingkan query dengan passages yang ada dalam satu baris."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8ac577",
      "metadata": {
        "id": "da8ac577"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "average_precisions_tfidf = []\n",
        "\n",
        "for index, row in df_train.iterrows():\n",
        "    query = row['query']\n",
        "    passages = row['passages']['passage_text']\n",
        "    y_true = np.array(row['passages']['is_selected'])\n",
        "\n",
        "    if not passages:\n",
        "        average_precisions_tfidf.append(0.0)\n",
        "        continue\n",
        "\n",
        "    # Fit and transform for the current query and its passages\n",
        "    try:\n",
        "        all_texts = [query] + passages\n",
        "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "        query_vector = tfidf_matrix[0]\n",
        "        passage_vectors = tfidf_matrix[1:]\n",
        "\n",
        "        y_scores = [cos_sim(query_vector.toarray().flatten(), passage_vector.toarray().flatten()) for passage_vector in passage_vectors]\n",
        "        average_precisions_tfidf.append(average_precision(y_true, np.array(y_scores)))\n",
        "    except ValueError: # Happens if vocabulary is empty (e.g. all stopwords)\n",
        "        average_precisions_tfidf.append(0.0)\n",
        "\n",
        "map_tfidf = np.mean(average_precisions_tfidf)\n",
        "print(f\"MAP with TF-IDF: {map_tfidf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8da923",
      "metadata": {
        "id": "ea8da923"
      },
      "source": [
        "### Soal 1.5 (2 poin)\n",
        "\n",
        "Implementasikan sistem yang serupa seperti soal 1.3, tetapi kali ini gunakan BM25 untuk menghitung skornya. Gunakan implementasi seperti yang dicontohkan [di sini](https://pypi.org/project/rank-bm25/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bce068",
      "metadata": {
        "id": "97bce068"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "average_precisions_bm25 = []\n",
        "\n",
        "for index, row in df_train.iterrows():\n",
        "    query = row['query']\n",
        "    passages = row['passages']['passage_text']\n",
        "    y_true = np.array(row['passages']['is_selected'])\n",
        "\n",
        "    if not passages:\n",
        "        average_precisions_bm25.append(0.0)\n",
        "        continue\n",
        "\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in passages]\n",
        "    tokenized_query = query.split(\" \")\n",
        "\n",
        "    try:\n",
        "        bm25 = BM25Okapi(tokenized_corpus)\n",
        "        y_scores = bm25.get_scores(tokenized_query)\n",
        "        average_precisions_bm25.append(average_precision(y_true, y_scores))\n",
        "    except ValueError: # Can happen with empty documents or queries after tokenization\n",
        "        average_precisions_bm25.append(0.0)\n",
        "    except IndexError: # Can happen if all documents are empty, BM25 can't initialize\n",
        "        average_precisions_bm25.append(0.0)\n",
        "\n",
        "map_bm25 = np.mean(average_precisions_bm25)\n",
        "print(f\"MAP with BM25: {map_bm25}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32939d7b",
      "metadata": {
        "id": "32939d7b"
      },
      "source": [
        "### Soal 1.6 (3 poin)\n",
        "\n",
        "Bagaimana nilai MAP yang Anda dapatkan dari BM25 jika dibandingkan dengan tf-idf? Mengapa hasilnya seperti itu?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb6e70b",
      "metadata": {
        "id": "afb6e70b"
      },
      "source": [
        "Nilai MAP yang dihasilkan oleh BM25 umumnya lebih tinggi dibandingkan dengan TF-IDF. Hal ini disebabkan oleh beberapa faktor:\n",
        "1.  **Penanganan Panjang Dokumen**: BM25 secara eksplisit memperhitungkan panjang dokumen dalam perhitungannya. Dokumen yang lebih pendek yang mengandung term pencarian akan mendapatkan skor yang lebih tinggi dibandingkan dokumen panjang dengan frekuensi term yang sama. TF-IDF tidak secara langsung mengakomodasi ini dengan cara yang sama.\n",
        "2.  **Saturasi Frekuensi Term**: BM25 memiliki komponen saturasi frekuensi term (parameter k1). Ini berarti bahwa setelah frekuensi term mencapai titik tertentu, peningkatan lebih lanjut dalam frekuensi term tersebut memberikan peningkatan skor yang semakin berkurang. Hal ini membantu mencegah term yang sangat umum mendominasi skor relevansi secara tidak proporsional, yang bisa terjadi pada TF-IDF.\n",
        "3.  **Parameter yang Dapat Disesuaikan**: BM25 memiliki parameter (k1 dan b) yang dapat disesuaikan (tuned) untuk dataset tertentu, yang memungkinkannya beradaptasi lebih baik dengan karakteristik data tersebut. TF-IDF memiliki lebih sedikit fleksibilitas dalam hal ini.\n",
        "\n",
        "Secara umum, BM25 dianggap sebagai penyempurnaan dari model probabilitas dan seringkali memberikan hasil yang lebih baik dalam tugas-tugas pencarian informasi dibandingkan TF-IDF standar, terutama ketika panjang dokumen bervariasi dan ada kebutuhan untuk mengontrol dampak frekuensi term yang sangat tinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a22cde03",
      "metadata": {
        "id": "a22cde03"
      },
      "source": [
        "## 2. Dense Matrix Retrieval (10 poin)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c853d79",
      "metadata": {
        "id": "2c853d79"
      },
      "source": [
        "### Soal 2.1 (2 poin)\n",
        "\n",
        "Ambil 500 sampel (baris) dari `df_val`. Hitung nilai MAP dari sistem dengan tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e4cfb9",
      "metadata": {
        "id": "46e4cfb9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "df_sample = df_val.sample(n=500, random_state=42)\n",
        "\n",
        "average_precisions_tfidf_sample = []\n",
        "vectorizer_sample = TfidfVectorizer()\n",
        "\n",
        "for index, row in df_sample.iterrows():\n",
        "    query = row['query']\n",
        "    passages = row['passages']['passage_text']\n",
        "    y_true = np.array(row['passages']['is_selected'])\n",
        "\n",
        "    if not passages:\n",
        "        average_precisions_tfidf_sample.append(0.0)\n",
        "        continue\n",
        "    try:\n",
        "        all_texts = [query] + passages\n",
        "        tfidf_matrix = vectorizer_sample.fit_transform(all_texts)\n",
        "        query_vector = tfidf_matrix[0]\n",
        "        passage_vectors = tfidf_matrix[1:]\n",
        "\n",
        "        y_scores = [cos_sim(query_vector.toarray().flatten(), passage_vector.toarray().flatten()) for passage_vector in passage_vectors]\n",
        "        average_precisions_tfidf_sample.append(average_precision(y_true, np.array(y_scores)))\n",
        "    except ValueError:\n",
        "        average_precisions_tfidf_sample.append(0.0)\n",
        "\n",
        "map_tfidf_sample = np.mean(average_precisions_tfidf_sample)\n",
        "print(f\"MAP with TF-IDF on df_sample: {map_tfidf_sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf85fe3",
      "metadata": {
        "id": "3cf85fe3"
      },
      "source": [
        "### Soal 2.2 (3 poin)\n",
        "\n",
        "Gunakan model `all-MiniLM-L6-v2` untuk menghasilkan embeddings dari query dan passages. Lalu, hitung nilai MAP dari sampel yang dihasilkan di soal 2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb25eb9",
      "metadata": {
        "id": "acb25eb9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "average_precisions_minilm_sample = []\n",
        "\n",
        "for index, row in df_sample.iterrows():\n",
        "    query = row['query']\n",
        "    passages = row['passages']['passage_text']\n",
        "    y_true = np.array(row['passages']['is_selected'])\n",
        "\n",
        "    if not passages:\n",
        "        average_precisions_minilm_sample.append(0.0)\n",
        "        continue\n",
        "\n",
        "    query_embedding = model_minilm.encode(query)\n",
        "    passage_embeddings = model_minilm.encode(passages)\n",
        "\n",
        "    y_scores = [cos_sim(query_embedding, passage_embedding) for passage_embedding in passage_embeddings]\n",
        "    average_precisions_minilm_sample.append(average_precision(y_true, np.array(y_scores)))\n",
        "\n",
        "map_minilm_sample = np.mean(average_precisions_minilm_sample)\n",
        "print(f\"MAP with all-MiniLM-L6-v2 on df_sample: {map_minilm_sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae3ab63",
      "metadata": {
        "id": "2ae3ab63"
      },
      "source": [
        "### Soal 2.3 (3 poin)\n",
        "\n",
        "Bagaimana nilai MAP yang Anda dapatkan dari sistem dengan embeddings jika dibandingkan dengan tf-idf? Tuliskan 3 kelebihan dari sistem IR dengan embeddings jika dibandingkan dengan tf-idf."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89333f9f",
      "metadata": {
        "id": "89333f9f"
      },
      "source": [
        "Nilai MAP yang didapatkan dari sistem dengan embeddings (menggunakan `all-MiniLM-L6-v2`) kemungkinan besar akan lebih tinggi dibandingkan dengan TF-IDF. Ini karena model embedding mampu menangkap makna semantik dan hubungan kontekstual antar kata, tidak hanya berdasarkan frekuensi kata.\n",
        "\n",
        "Berikut adalah 3 kelebihan sistem IR dengan embeddings dibandingkan dengan TF-IDF:\n",
        "1.  **Pemahaman Semantik**: Embeddings dapat memahami sinonim dan parafrase. Misalnya, query \"mobil murah\" mungkin akan menemukan dokumen yang berisi \"kendaraan ekonomis\" karena model embedding memahami bahwa kedua frasa tersebut memiliki makna yang serupa. TF-IDF akan kesulitan menangkap hubungan ini karena hanya melihat kesamaan kata secara literal.\n",
        "2.  **Penanganan Kata di Luar Kosakata (Out-of-Vocabulary/OOV)**: Model embedding modern, terutama yang berbasis subword (seperti BPE atau WordPiece), dapat menangani kata-kata yang tidak ada dalam vocabulary pelatihan dengan lebih baik. Mereka dapat merepresentasikan kata-kata baru berdasarkan subword yang sudah dikenal. TF-IDF akan mengabaikan kata-kata OOV atau memperlakukannya sebagai token unik tanpa informasi semantik.\n",
        "3.  **Representasi Kontekstual**: Embeddings, terutama yang dihasilkan oleh model Transformer, dapat menghasilkan representasi kata atau kalimat yang berbeda tergantung pada konteksnya. Ini berarti kata yang sama bisa memiliki embedding yang berbeda jika digunakan dalam makna yang berbeda. TF-IDF tidak memiliki kemampuan ini; setiap kata memiliki representasi yang tetap terlepas dari konteks penggunaannya."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f23430",
      "metadata": {
        "id": "72f23430"
      },
      "source": [
        "### Soal 2.4 (2 poin)\n",
        "\n",
        "Karena dataset yang digunakan sudah menangani bagian *retrieval*, gunakan model `CrossEncoder` untuk memberikan skor dari passages yang ada. Hitung nilai MAP-nya. Apakah Anda mendapatkan nilai MAP yang lebih baik?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cce487",
      "metadata": {
        "id": "f3cce487"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "average_precisions_cross_encoder_sample = []\n",
        "\n",
        "for index, row in df_sample.iterrows():\n",
        "    query = row['query']\n",
        "    passages = row['passages']['passage_text']\n",
        "    y_true = np.array(row['passages']['is_selected'])\n",
        "\n",
        "    if not passages:\n",
        "        average_precisions_cross_encoder_sample.append(0.0)\n",
        "        continue\n",
        "\n",
        "    # CrossEncoder expects a list of [query, passage] pairs\n",
        "    sentence_pairs = [[query, passage] for passage in passages]\n",
        "    y_scores = cross_encoder_model.predict(sentence_pairs)\n",
        "\n",
        "    average_precisions_cross_encoder_sample.append(average_precision(y_true, y_scores))\n",
        "\n",
        "map_cross_encoder_sample = np.mean(average_precisions_cross_encoder_sample)\n",
        "print(f\"MAP with CrossEncoder on df_sample: {map_cross_encoder_sample}\")\n",
        "\n",
        "print(f\"\\nComparison of MAP scores on df_sample:\")\n",
        "print(f\"TF-IDF: {map_tfidf_sample}\")\n",
        "print(f\"all-MiniLM-L6-v2 (Bi-Encoder): {map_minilm_sample}\")\n",
        "print(f\"Cross-Encoder: {map_cross_encoder_sample}\")\n",
        "\n",
        "print(\"\\nNilai MAP yang didapatkan dengan CrossEncoder kemungkinan besar akan menjadi yang tertinggi. CrossEncoder memproses query dan passage secara bersamaan, memungkinkan interaksi yang lebih dalam antar token dan pemahaman konteks yang lebih baik dibandingkan Bi-Encoder (seperti all-MiniLM-L6-v2) yang menghasilkan embedding secara terpisah. Meskipun lebih lambat karena harus memproses setiap pasangan query-passage, akurasinya cenderung lebih superior untuk tugas re-ranking.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fcbcb00",
      "metadata": {
        "id": "6fcbcb00"
      },
      "source": [
        "## 3. Retrieval-Augmented Generation (25 poin)\n",
        "\n",
        "*Acknowledgement:* Data yang digunakan pada bagian ini dikumpulkan oleh [Miguel Escobar Varela](https://miguelescobar.com/) dan tersedia secara terbuka di [HuggingFace](https://huggingface.co/datasets/mevsg/warisan-classification-data-v1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5dabac1",
      "metadata": {
        "id": "f5dabac1"
      },
      "outputs": [],
      "source": [
        "splits = {\n",
        "    \"train\": \"data/train-00000-of-00001.parquet\",\n",
        "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
        "}\n",
        "df = pd.concat(\n",
        "    [\n",
        "        pd.read_parquet(\n",
        "            \"hf://datasets/mevsg/warisan-classification-data-v1/\" + splits[\"train\"]\n",
        "        ),\n",
        "        pd.read_parquet(\n",
        "            \"hf://datasets/mevsg/warisan-classification-data-v1/\" + splits[\"test\"]\n",
        "        ),\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2747e8",
      "metadata": {
        "id": "eb2747e8"
      },
      "source": [
        "### Soal 3.1 (3 poin)\n",
        "\n",
        "Dengan menggunakan model `Llama-3.2-1B`, berikan pertanyaan berikut sebagai prompt dan tampilkan jawabannya. Apa yang dapat Anda amati dari hasilnya?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e5e897",
      "metadata": {
        "id": "c4e5e897"
      },
      "outputs": [],
      "source": [
        "query = \"Apa saja warisan budaya tak benda dari Jawa Barat?\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer_llama(query, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "with torch.no_grad():\n",
        "    outputs = model_llama.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer_llama.eos_token_id) # Added pad_token_id\n",
        "\n",
        "response = tokenizer_llama.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Llama-3.2-1B Response (without RAG):\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbdd6ae1",
      "metadata": {
        "id": "dbdd6ae1"
      },
      "source": [
        "Jawaban dari model Llama-3.2-1B tanpa RAG kemungkinan akan bersifat umum atau bahkan menghasilkan informasi yang tidak akurat (halusinasi). Model ini, meskipun besar, mungkin tidak memiliki pengetahuan spesifik dan terkini mengenai daftar warisan budaya tak benda dari Jawa Barat yang ada dalam dataset `warisan-classification-data-v1`.\n",
        "\n",
        "Pengamatan yang mungkin muncul:\n",
        "1.  **Jawaban Generik**: Model mungkin memberikan contoh-contoh umum warisan budaya Indonesia atau Jawa secara umum, tanpa menyebutkan secara spesifik yang terdaftar resmi atau yang ada di dataset.\n",
        "2.  **Ketidakakuratan/Halusinasi**: Model bisa saja menyebutkan beberapa item yang terdengar plausibel namun sebenarnya bukan warisan budaya tak benda dari Jawa Barat, atau bahkan mengarang nama-nama warisan.\n",
        "3.  **Kurangnya Detail**: Jawaban mungkin tidak mendalam dan tidak memberikan konteks atau penjelasan lebih lanjut mengenai warisan yang disebutkan.\n",
        "4.  **Ketergantungan pada Data Pelatihan**: Jawaban sangat bergantung pada data yang digunakan untuk melatih model Llama tersebut. Jika data pelatihan tidak mencakup informasi detail tentang warisan budaya Jawa Barat, maka model tidak akan bisa memberikannya.\n",
        "\n",
        "Hal ini menunjukkan keterbatasan LLM standar ketika dihadapkan pada pertanyaan yang membutuhkan pengetahuan domain spesifik yang mungkin tidak tercakup secara komprehensif dalam data pelatihannya. Ini menjadi dasar mengapa RAG diperlukan, yaitu untuk menyediakan konteks yang relevan dan faktual kepada LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f61b3b",
      "metadata": {
        "id": "95f61b3b"
      },
      "source": [
        "### Soal 3.2 (2 poin)\n",
        "\n",
        "Sekarang, Anda akan membangun sistem retrieval-augmented generation (RAG) sederhana. Bagian pertama dari model ini adalah membuat embeddings dari corpus yang kita gunakan.\n",
        "\n",
        "Bentuk corpus dari kolom `texts`. Urai teks menggunakan delimiter `\\n\\n`, simpan hanya teks dengan ukuran lebih dari 100 karakter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33c1bf0",
      "metadata": {
        "id": "c33c1bf0"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for text_entry in df['texts']:\n",
        "    paragraphs = text_entry.split('\\n\\n')\n",
        "    for paragraph in paragraphs:\n",
        "        if len(paragraph) > 100:\n",
        "            corpus.append(paragraph.strip())\n",
        "\n",
        "print(f\"Jumlah dokumen dalam korpus: {len(corpus)}\")\n",
        "assert len(corpus) == 12037"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462042ea",
      "metadata": {
        "id": "462042ea"
      },
      "source": [
        "### Soal 3.3.a (2 poin)\n",
        "\n",
        "Buat embeddings dari corpus dengan model `all-MiniLM-L6-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1a5f98",
      "metadata": {
        "id": "0d1a5f98"
      },
      "outputs": [],
      "source": [
        "model_minilm_rag = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "corpus_embeddings_minilm = model_minilm_rag.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
        "print(f\"Shape of MiniLM corpus embeddings: {corpus_embeddings_minilm.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dbca98",
      "metadata": {
        "id": "d5dbca98"
      },
      "source": [
        "### Soal 3.3.b (2 poin)\n",
        "\n",
        "Buat embeddings dari corpus dengan model `intfloat/multilingual-e5-small`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77deaeb7",
      "metadata": {
        "id": "77deaeb7"
      },
      "outputs": [],
      "source": [
        "model_e5_rag = SentenceTransformer('intfloat/multilingual-e5-small')\n",
        "corpus_embeddings_e5 = model_e5_rag.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
        "print(f\"Shape of E5 corpus embeddings: {corpus_embeddings_e5.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0eef526",
      "metadata": {
        "id": "b0eef526"
      },
      "source": [
        "### Soal 3.3.c (4 poin)\n",
        "\n",
        "Buat embedding dari variabel `query` dengan menggunakan dua model dari soal 3.3.a dan 3.3.b, lalu ambil lima dokumen dengan skor paling tinggi dari `corpus`. Anda dapat memanfaatkan fungsi *semantic search* dari Sentence Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06364352",
      "metadata": {
        "id": "06364352"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "query_rag = \"Apa saja warisan budaya tak benda dari Jawa Barat?\"\n",
        "top_k = 5\n",
        "\n",
        "# Semantic Search with all-MiniLM-L6-v2\n",
        "query_embedding_minilm = model_minilm_rag.encode(query_rag, convert_to_tensor=True)\n",
        "hits_minilm = util.semantic_search(query_embedding_minilm, corpus_embeddings_minilm, top_k=top_k)\n",
        "hits_minilm = hits_minilm[0]  # Get the hits for the first query\n",
        "\n",
        "print(f\"Query: {query_rag}\")\n",
        "print(f\"\\nTop {top_k} results from all-MiniLM-L6-v2:\")\n",
        "for hit in hits_minilm:\n",
        "    print(f\"  Score: {hit['score']:.4f} - {corpus[hit['corpus_id']]}\\n\")\n",
        "\n",
        "# Semantic Search with intfloat/multilingual-e5-small\n",
        "query_embedding_e5 = model_e5_rag.encode(query_rag, convert_to_tensor=True)\n",
        "hits_e5 = util.semantic_search(query_embedding_e5, corpus_embeddings_e5, top_k=top_k)\n",
        "hits_e5 = hits_e5[0]  # Get the hits for the first query\n",
        "\n",
        "print(f\"\\nTop {top_k} results from intfloat/multilingual-e5-small:\")\n",
        "for hit in hits_e5:\n",
        "    print(f\"  Score: {hit['score']:.4f} - {corpus[hit['corpus_id']]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "441ad722",
      "metadata": {
        "id": "441ad722"
      },
      "source": [
        "### Soal 3.3.d (2 poin)\n",
        "\n",
        "Apa yang dapat Anda amati dari hasil kedua model di atas? Apa yang menyebabkan perbedaan tersebut?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e18add26",
      "metadata": {
        "id": "e18add26"
      },
      "source": [
        "Perbedaan hasil antara `all-MiniLM-L6-v2` dan `intfloat/multilingual-e5-small` dapat disebabkan oleh beberapa faktor utama:\n",
        "\n",
        "1.  **Data Pelatihan dan Kemampuan Multibahasa**:\n",
        "    *   `all-MiniLM-L6-v2` adalah model yang sangat baik untuk bahasa Inggris dan dilatih pada dataset berbahasa Inggris yang besar. Meskipun dapat menangani bahasa lain sampai batas tertentu melalui kemiripan token atau transfer learning implisit, performanya mungkin tidak seoptimal pada bahasa non-Inggris.\n",
        "    *   `intfloat/multilingual-e5-small` secara eksplisit dirancang dan dilatih untuk menjadi model multibahasa. Model ini dilatih pada data dari banyak bahasa, termasuk bahasa Indonesia. Oleh karena itu, untuk query dan korpus dalam bahasa Indonesia (seperti dalam kasus ini), `multilingual-e5-small` cenderung memiliki pemahaman semantik yang lebih baik dan representasi yang lebih akurat.\n",
        "\n",
        "2.  **Arsitektur Model dan Ukuran**:\n",
        "    *   Meskipun keduanya adalah model Transformer, perbedaan dalam arsitektur spesifik, jumlah parameter (Meskipun `multilingual-e5-small` adalah versi 'small', arsitektur E5 mungkin berbeda dari MiniLM), dan detail pelatihan lainnya dapat memengaruhi kualitas embedding yang dihasilkan untuk tugas tertentu.\n",
        "\n",
        "3.  **Spesialisasi Tugas Pelatihan**:\n",
        "    *   Model E5 (Embedding for Everything) dilatih dengan tujuan untuk menghasilkan embedding yang baik untuk berbagai tugas, termasuk pencarian semantik (retrieval). Cara pelatihannya mungkin lebih dioptimalkan untuk membedakan antara query dan passage yang relevan dalam konteks multibahasa.\n",
        "\n",
        "**Observasi yang Diharapkan**:\n",
        "*   **Relevansi Hasil**: Kemungkinan besar `intfloat/multilingual-e5-small` akan memberikan hasil pencarian yang lebih relevan untuk query berbahasa Indonesia \"Apa saja warisan budaya tak benda dari Jawa Barat?\". Dokumen yang diambil mungkin lebih akurat mencerminkan konten yang berkaitan dengan Jawa Barat dan warisan budayanya.\n",
        "*   **Skor Kepercayaan (Score)**: Skor yang diberikan oleh `multilingual-e5-small` mungkin lebih terkalibrasi dengan baik untuk relevansi dalam konteks bahasa Indonesia.\n",
        "*   **Nuansa Bahasa**: `multilingual-e5-small` mungkin lebih mampu menangkap nuansa bahasa Indonesia dan istilah-istilah spesifik yang terkait dengan budaya lokal dibandingkan `all-MiniLM-L6-v2` yang fokus utama pelatihannya adalah bahasa Inggris.\n",
        "\n",
        "Secara singkat, karena query dan korpusnya dalam bahasa Indonesia, model yang secara eksplisit dilatih untuk multibahasa seperti `intfloat/multilingual-e5-small` diharapkan memberikan performa yang lebih baik dalam hal relevansi pencarian semantik."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74504641",
      "metadata": {
        "id": "74504641"
      },
      "source": [
        "### Soal 3.4 (3 poin)\n",
        "\n",
        "Dengan menggunakan model `intfloat/multilingual-e5-small` untuk mencari dokumen yang relevan, rangkum hasilnya dengan model `Llama-3.2-1B`.\n",
        "\n",
        "Catatan: Anda mungkin butuh rekayasa *prompt* agar mendapatkan hasil yang diharapkan, e.g. membuang informasi yang tidak relevan, rangkuman tertulis dalam format yang rapi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518bbbc7",
      "metadata": {
        "id": "518bbbc7"
      },
      "outputs": [],
      "source": [
        "retrieved_passages_e5 = [corpus[hit['corpus_id']] for hit in hits_e5]\n",
        "context_for_llama = \"\\n\\n\".join(retrieved_passages_e5)\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "Berdasarkan informasi berikut:\n",
        "{context}\n",
        "\n",
        "Jawab pertanyaan ini: {question}\n",
        "Berikan jawaban yang ringkas dan hanya berdasarkan informasi yang diberikan. Sebutkan beberapa warisan budaya tak benda dari Jawa Barat.\n",
        "\"\"\"\n",
        "\n",
        "prompt_rag = prompt_template.format(context=context_for_llama, question=query_rag)\n",
        "\n",
        "inputs_rag = tokenizer_llama(prompt_rag, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text with RAG\n",
        "with torch.no_grad():\n",
        "    outputs_rag = model_llama.generate(**inputs_rag, max_new_tokens=250, pad_token_id=tokenizer_llama.eos_token_id, temperature=0.7, do_sample=True) # Added temperature and do_sample for better generation\n",
        "\n",
        "response_rag = tokenizer_llama.decode(outputs_rag[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Query: {query_rag}\")\n",
        "print(f\"\\nLlama-3.2-1B Response (with RAG using multilingual-e5-small):\\n{response_rag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f5c909",
      "metadata": {
        "id": "b9f5c909"
      },
      "source": [
        "### Soal 3.5 (2 poin)\n",
        "\n",
        "Jelaskan dua contoh metrik evaluasi yang dapat digunakan untuk mengukur kualitas bagian generation dari sebuah sistem RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2dd666",
      "metadata": {
        "id": "4e2dd666"
      },
      "source": [
        "Berikut adalah dua contoh metrik evaluasi yang dapat digunakan untuk mengukur kualitas bagian *generation* dari sebuah sistem RAG, beserta penjelasannya:\n",
        "\n",
        "1.  **BLEU (Bilingual Evaluation Understudy)**:\n",
        "    *   **Deskripsi**: BLEU adalah metrik yang awalnya dirancang untuk mengevaluasi kualitas terjemahan mesin, tetapi juga sering digunakan untuk mengevaluasi teks yang dihasilkan oleh model generatif lainnya, termasuk dalam RAG. BLEU bekerja dengan membandingkan n-gram (urutan kata yang berdekatan) dari teks yang dihasilkan model dengan n-gram dari satu atau lebih referensi teks (jawaban ideal yang ditulis manusia). Skor BLEU berkisar antara 0 dan 1 (atau 0 hingga 100), di mana skor yang lebih tinggi menunjukkan kesamaan yang lebih besar dengan referensi.\n",
        "    *   **Kelebihan**: Cepat dihitung, banyak digunakan sehingga mudah untuk membandingkan dengan penelitian lain, dan berkorelasi cukup baik dengan penilaian manusia untuk beberapa tugas.\n",
        "    *   **Keterbatasan**: Sangat bergantung pada kualitas referensi. Kurang baik dalam menangkap makna semantik atau parafrase (jika teks yang dihasilkan menggunakan kata-kata berbeda tetapi maknanya sama dengan referensi, skor BLEU bisa rendah). Juga, tidak memperhitungkan kelancaran (fluency) atau koherensi secara langsung.\n",
        "    *   **Penggunaan dalam RAG**: Digunakan untuk menilai seberapa dekat jawaban yang dihasilkan oleh LLM (setelah diberi konteks dari retriever) dengan jawaban referensi yang dianggap benar dan relevan dengan query serta konteks yang diberikan.\n",
        "\n",
        "2.  **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:\n",
        "    *   **Deskripsi**: ROUGE adalah serangkaian metrik yang digunakan untuk mengevaluasi ringkasan otomatis dan juga relevan untuk teks yang dihasilkan secara generatif. Berbeda dengan BLEU yang fokus pada presisi n-gram, ROUGE lebih fokus pada recall n-gram. Varian yang umum digunakan meliputi:\n",
        "        *   `ROUGE-N`: Mengukur tumpang tindih n-gram (misalnya, `ROUGE-1` untuk unigram, `ROUGE-2` untuk bigram).\n",
        "        *   `ROUGE-L`: Mengukur Longest Common Subsequence (LCS) antara teks yang dihasilkan dan referensi. Ini menangkap kesamaan struktur kalimat.\n",
        "    *   **Kelebihan**: Baik dalam menilai seberapa banyak informasi penting dari referensi yang muncul dalam teks yang dihasilkan. `ROUGE-L` dapat menangkap kesamaan urutan kata meskipun tidak persis sama.\n",
        "    *   **Keterbatasan**: Seperti BLEU, sangat bergantung pada referensi. Mungkin tidak sepenuhnya menangkap kualitas semantik jika kata-kata yang digunakan berbeda. Kurang menilai aspek seperti faktualitas (jika tidak ada dalam referensi) atau kelancaran.\n",
        "    *   **Penggunaan dalam RAG**: Digunakan untuk menilai apakah jawaban yang dihasilkan oleh LLM mencakup poin-poin penting yang diharapkan ada dalam jawaban berdasarkan query dan konteks yang diambil. Ini membantu mengukur sejauh mana LLM berhasil memanfaatkan informasi dari dokumen yang diambil untuk menghasilkan jawaban yang komprehensif.\n",
        "\n",
        "Metrik lain yang juga bisa dipertimbangkan termasuk **METEOR**, **CIDEr** (untuk deskripsi gambar, tapi konsepnya bisa diadaptasi), atau metrik berbasis embedding seperti **BERTScore** yang mencoba menangkap kesamaan semantik lebih baik daripada metrik berbasis n-gram tradisional. Evaluasi manusia juga tetap menjadi standar emas untuk menilai kualitas generasi teks secara menyeluruh."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18417d95",
      "metadata": {
        "id": "18417d95"
      },
      "source": [
        "### Soal 3.6 (5 poin)\n",
        "\n",
        "Lakukan eksplorasi tambahan untuk bagian ini, misalnya:\n",
        "1. Evaluasi sistem Anda dengan query yang berbeda\n",
        "2. Gunakan model berbeda untuk bagian IR dari sistem\n",
        "3. Gunakan model berbeda sebagai LLM\n",
        "4. Gunakan OpenAI API tanpa komponen IR untuk mengevaluasi query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81bd40cb",
      "metadata": {
        "id": "81bd40cb"
      },
      "outputs": [],
      "source": [
        "# Eksplorasi Tambahan (Contoh)\n",
        "\n",
        "# 1. Evaluasi sistem dengan query yang berbeda\n",
        "new_query = \"Sebutkan beberapa tarian tradisional dari Sumatera Utara.\"\n",
        "print(f\"--- Mengevaluasi dengan Query Baru: '{new_query}' ---\")\n",
        "\n",
        "#   a. Dapatkan konteks dengan model retrieval (multilingual-e5-small)\n",
        "new_query_embedding_e5 = model_e5_rag.encode(new_query, convert_to_tensor=True)\n",
        "new_hits_e5 = util.semantic_search(new_query_embedding_e5, corpus_embeddings_e5, top_k=top_k)\n",
        "new_hits_e5 = new_hits_e5[0]\n",
        "retrieved_passages_new_query = [corpus[hit['corpus_id']] for hit in new_hits_e5]\n",
        "context_for_llama_new_query = \"\\n\\n\".join(retrieved_passages_new_query)\n",
        "\n",
        "print(f\"Dokumen yang diambil untuk query baru:\")\n",
        "for i, passage in enumerate(retrieved_passages_new_query):\n",
        "    print(f\"  {i+1}. {passage[:150]}...\") # Print first 150 chars\n",
        "\n",
        "#   b. Hasilkan jawaban dengan Llama-3.2-1B menggunakan konteks baru\n",
        "prompt_rag_new_query = prompt_template.format(context=context_for_llama_new_query, question=new_query)\n",
        "inputs_rag_new_query = tokenizer_llama(prompt_rag_new_query, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_rag_new_query = model_llama.generate(**inputs_rag_new_query, max_new_tokens=250, pad_token_id=tokenizer_llama.eos_token_id, temperature=0.7, do_sample=True)\n",
        "\n",
        "response_rag_new_query = tokenizer_llama.decode(outputs_rag_new_query[0], skip_special_tokens=True)\n",
        "print(f\"\\nLlama-3.2-1B Response untuk query baru (with RAG):\")\n",
        "print(response_rag_new_query)\n",
        "\n",
        "# Komentar Tambahan:\n",
        "# Untuk eksplorasi lebih lanjut, bisa juga:\n",
        "# 2. Gunakan model berbeda untuk IR: Misalnya, coba BM25 atau TF-IDF lagi pada korpus 'warisan' dan bandingkan dokumen yang diambilnya.\n",
        "# 3. Gunakan model LLM berbeda: Jika akses tersedia, coba model seperti Llama-3-8B atau model lain yang lebih besar/kecil untuk melihat perbedaan kualitas generasi.\n",
        "# 4. Gunakan OpenAI API tanpa RAG: Kirim 'new_query' ke API OpenAI (misal GPT-3.5 atau GPT-4) tanpa memberikan konteks dari korpus kita, lalu bandingkan jawabannya dengan hasil RAG.\n",
        "#    Ini akan menunjukkan seberapa baik pengetahuan umum model tersebut dibandingkan dengan sistem RAG yang memiliki akses ke data spesifik.\n",
        "#    Contoh (membutuhkan setup API Key OpenAI):\n",
        "#    # from openai import OpenAI\n",
        "#    # client = OpenAI(api_key='YOUR_OPENAI_API_KEY')\n",
        "#    # response_openai = client.chat.completions.create(\n",
        "#    #    model=\"gpt-3.5-turbo\",\n",
        "#    #    messages=[\n",
        "#    #        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#    #        {\"role\": \"user\", \"content\": new_query}\n",
        "#    #    ]\n",
        "#    # )\n",
        "#    # print(f\"\\nOpenAI GPT-3.5 Response (without RAG):\\n{response_openai.choices[0].message.content}\")\n",
        "print(\"\\nEksplorasi di atas menunjukkan bagaimana sistem RAG merespons query yang berbeda dengan mengambil konteks yang relevan dari korpus yang sama dan menggunakan LLM untuk menghasilkan jawaban berdasarkan konteks tersebut.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}